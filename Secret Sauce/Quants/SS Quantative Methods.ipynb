{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b580476-99df-478b-abf5-0e8e63d9f1eb",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# SECRET SAUCE\n",
    "\n",
    "# Topic Focus: Quantitative Methods\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Weight on Exam</td>\n",
    "    <td>6% to 9%</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>SchweserNotes™ Reference</td>\n",
    "    <td>Book 1, Pages 1–136</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<p><br></p>\n",
    "Understanding time value of money (TVM) computations is essential for success, not only for quantitative methods, but also other sections of the Level I exam. Candidates who are unfamiliar with TVM calculations, or simply need a refresher, should review them in the CFA Institute prerequisite material for Quantitative Methods. TVM is actually a larger portion of the exam than just quantitative methods because of its integration with other topics. Any portion of the exam that requires discounting cash flows will require TVM calculations. Examples include evaluating capital projects, valuing bonds, and using dividend discount models to value equities. No matter where it shows up on the exam, the key to any TVM problem is to draw a timeline and be certain of when the cash flows will occur so you can discount those cash flows appropriately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876aeac3-9c02-4c6f-b225-f62885a65757",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# SECRET SAUCE\n",
    "\n",
    "# Reading 1: Rates and Returns\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5838a65-5de1-4aa2-97e5-954763af7d19",
   "metadata": {},
   "source": [
    "Interest rates measure the time value of money. Equilibrium interest rates are the **required rate of return** for a particular investment. Interest rates are also referred to as **discount rates**. We can also view interest rates as the opportunity cost of current consumption.\n",
    "\n",
    "The **real risk-free rate** is a theoretical interest rate on a single-period loan with no risk of inflation or default. The real risk-free rate represents **time preference**, the degree to which current consumption is preferred to equal future consumption.\n",
    "\n",
    "A **nominal risk-free rate** contains an inflation premium:\n",
    "\n",
    "        (1 + nominal risk-free rate) = (1 + real risk-free rate)(1 + expected inflation rate)\n",
    "\n",
    "This relation is approximated as:\n",
    "\n",
    "        nominal risk-free rate ≈ real risk-free rate + expected inflation rate.\n",
    "\n",
    "Risk increases the required rate of return. Types of risks include **default risk**, that a borrower will not make the promised payments on time; **liquidity risk** of receiving less than fair value if an investment must be sold quickly; and **maturity risk**, because the prices of longer-term bonds are more volatile than those of shorter-term bonds. Thus, we can state:\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>nominal interest rate ≈</td>\n",
    "    <td>real risk-free rate</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>+ inflation premium</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>+ default risk premium</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>+ liquidity risk premium</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>+ maturity risk premium</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16084336-eb09-49f1-b2a1-e38d0b2c0f1c",
   "metadata": {},
   "source": [
    "### Return Measures\n",
    "\n",
    "A **holding period return (HPR)** is the percentage increase in the value of an investment over a given period.\n",
    "\n",
    "$$\n",
    "\\text{holding period return } = \\Large\\frac{\\text{end-of-period value + } \\text{CF}_1}{\\text{beginning-of-period value}}−1\n",
    "$$\n",
    "\\\n",
    "\\\n",
    "To **annualize** an HPR that is realized over a specific number of days, use the following formula:\n",
    "\n",
    "$$\n",
    "\\text{annualized return } = \\Large (1 + \\text{HPR})^{365/\\text{days}}–1\n",
    "$$\n",
    "\\\n",
    "\\\n",
    "The **arithmetic mean return** is the simple average of a series of periodic returns. \n",
    "problem with arithmetic mean *does not* reflect multiperiod compounding, \\\n",
    "$\\rightarrow$ Most appropriate for single period returns\n",
    "\n",
    "$$\n",
    "\\text{arithmetic mean return } = \\Large \\frac{R_1 + R_2 + R_3 + ... + R_n}{n}\n",
    "$$\n",
    "\n",
    "\\\n",
    "The **geometric mean return** is a compound rate:\n",
    "\\\n",
    "Geometric mean *does* reflect multi-period compounding.\n",
    "\\\n",
    "$\\rightarrow$ Most appropriate for average return over multiple periods \n",
    "\\\n",
    "$$\n",
    "\\text{geometric mean return } = \\Large \\sqrt[n]{(1+R_1)×(1+R_2)×...×(1+R_n)}-1\n",
    "$$\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "A **harmonic mean return** is used for certain computations, such as the average cost of shares purchased over time. \\\n",
    "Some refer to this practice as **cost averaging**.   *P/E* \\\n",
    "Harmonic mean gives lower weight to extreme observations (\"outliers\")\n",
    "\\\n",
    "$\\rightarrow$ Use harmonic mean when concerned about the impact of outliers\n",
    "\\\n",
    "$$\n",
    "\\Large \\frac{n}{\\frac{1}{X_1} + \\frac{1}{X_2} + \\frac{1}{X_3} +...+ \\frac{1}{X_n}}\n",
    "$$\n",
    "\n",
    " Returns data may include *outliers* , extreme values that distort mean return measures. A trimmed mean and a winsorized mean are techniques for dealing with outliers. A trimmed mean excludes a stated percentage of the most extreme observations. A 1% trimmed mean, for example, would discard the lowest 0.5% and the highest 0.5% of the observations. To compute a winsorized mean, instead of discarding the highest and lowest observations, we substitute a value for them. To calculate a 90% winsorized mean, for example, we would determine the 5th and 95th percentile of the observations, substitute the 5th percentile for any values lower than that, substitute the 95th percentile for any values higher than that, and then calculate the mean of the revised dataset.\n",
    "\n",
    "\n",
    "\n",
    "#### Comparing Averages\n",
    "\n",
    "* Harmonic mean ≤ geometric mean  ≤ arithmetic mean\n",
    "    * Difference in means is caused by *volalitilty* of data\n",
    "    * Means are equal when all values are the same  \n",
    "\n",
    "<p><br<</p>\n",
    "\n",
    "\n",
    "A **money-weighted return** is the internal rate of return on a portfolio that includes all cash inflows and outflows. The beginning value of the account and all deposits into the account are inflows. All withdrawals from the account and its ending value are outflows.\n",
    "\n",
    "Money-weighted returns are an **IRR** measure:  $\\rightarrow$ *does* reflect fund size.\n",
    "\\\n",
    "$$\n",
    "\\Large CF_0 + \\frac{CF_1}{1 + \\text{MWR}} + \\ldots + \\frac{CF_N}{1 + \\text{MWR}^N} = 0\n",
    "$$\n",
    "\\\n",
    "Periods must be equal length; use <u>shortest period with no significant cash flows</u>\n",
    "\n",
    "\n",
    "<p><br<</p>\n",
    "\n",
    "A **time-weighted return** measures compound growth over a specified time horizon. In the investment management industry, time-weighted return is the preferred method of performance measurement because portfolio managers typically do not control the timing of deposits to and withdrawals from the accounts they manage. The annual time-weighted return for an investment may be computed as follows:\\\n",
    "\\\n",
    "**Time weighted returns** $\\rightarrow$ *does not* reflect fund size.\n",
    "\n",
    "<table>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Step 1: Form subperiods that correspond to the dates of deposits and withdrawals.</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Step 2: Compute an HPR of the portfolio for each subperiod.</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Step 3: Compute the product of (1 + HPR) for each subperiod to obtain a total return for the entire measurement period, and annualize the result.</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "$$\n",
    "\\Large\\text{TWR} = \\Biggr[ \\Bigg(\\frac{\\text{end value}_1}{\\text{begin value}_1} \\Bigg) \\Bigg(\\frac{\\text{end value}_2}{\\text{begin value}_2} \\Bigg) \\ldots \\Bigg(\\frac{\\text{end value}_N}{\\text{begin value}_N} \\Bigg) \\Biggr] ^{\\frac{1}{\\text{YEARS}}} - 1\n",
    "$$\n",
    "\n",
    "If funds are contributed to an investment portfolio just before a period of relatively poor portfolio performance, the money-weighted rate of return will tend to be lower than the time-weighted rate of return. On the other hand, if funds are contributed to a portfolio at a favorable time (just before a period of relatively high returns), the money-weighted rate of return will be higher than the time-weighted rate of return.\n",
    "\n",
    "<p><br<</p>\n",
    "\n",
    "**Gross return** refers to the total return on a security portfolio before deducting fees for management and administration. \n",
    "\\\n",
    "\\\n",
    "**Net return** refers to the return after these fees have been deducted.\n",
    "\n",
    "<p><br<</p>\n",
    "\n",
    "**Pretax nominal return** refers to the return before paying taxes. \n",
    "\\\n",
    "\\\n",
    "**After-tax nominal return** refers to the return after the tax liability is deducted.\n",
    "\n",
    "<p><br<</p>\n",
    "\n",
    "**Real return** is nominal return adjusted for inflation. Consider a nominal return of `7%` in a year when inflation is `2%`. The approximate real return is `7% − 2% = 5%`. The exact real return is slightly lower: `1.07 / 1.02 − 1 = 4.9%`.\n",
    "\n",
    "<p><br<</p>\n",
    "\n",
    "**Leveraged return** is a gain or loss as a percentage of an investor's cash investment. For some investments, such as derivatives or real estate, the cash invested is only a fraction of the value of the underlying assets. Leverage magnifies both gains and losses on an investment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064749f-f311-42cc-8048-6653aec9c036",
   "metadata": {},
   "source": [
    "### Compounding Periods\n",
    "\n",
    "More frequent compounding increases the effective return on an investment, increasing the future value of a given cash flow and decreasing the present value of a given cash flow. The general formula for present value given a compounding frequency is as follows:\n",
    "\n",
    "$$\n",
    "\\Large PV=FV_N \\Big(1 + \\frac{r}{m}\\Big)^{–mN}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$r$ = quoted annual interest rate\n",
    "\n",
    "$N$ = number of years\n",
    "\n",
    "$m$ = compounding periods per year\n",
    "\n",
    "<p><br></p>\n",
    "\n",
    "The mathematical limit of shortening the compounding period is **continuous compounding**. We use the natural logarithm (ln) to state a continuously compounded rate of return:\n",
    "\n",
    "$$\n",
    "\\Large R_{\\text{CC}}=\\ln(1+\\text{HPR})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287bc6b-75c6-4186-b690-bab381d5194c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Reading 2: The Time Value of Money in Finance\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395f81a8-4e77-474c-b570-c2246b6cc706",
   "metadata": {},
   "source": [
    "With discrete compounding periods, the relationship between present values and future values is:\n",
    "\n",
    "$\\qquad FV = PV(1 + r)^t$   $\\quad \\text{or}$  $\\qquad PV = FV(1 + r)^{–t}$\n",
    "\n",
    "where:\n",
    "\n",
    "$r$ = interest rate per compounding period\n",
    "\n",
    "$t$ = number of compounding periods\n",
    "\n",
    "With continuous compounding, the relationship is:\n",
    "\n",
    "$\\qquad FV = PV × e^{rt}$  $\\quad \\text{or}$  $\\qquad PV = FV × e^{−rt}$\n",
    "\n",
    "Common applications of TVM in finance include valuing:\n",
    "\n",
    "* A single future cash flow, such as a zero-coupon bond.\n",
    "* A finite series of future cash flows, such as a coupon bond.\n",
    "* An infinite series of equal future cash flows, such as a perpetuity or a preferred stock.\n",
    "* A growing series of future cash flows, such as a common stock with a constant growth rate of dividends.\n",
    "\n",
    "A **zero-coupon bond** pays a single cash flow equal to its face value at maturity. Assuming a positive yield, the investor pays less than the face value to buy the bond.\n",
    "\n",
    "It is possible for the yield to be negative. In this case an investor would pay more than the face value to buy a zero-coupon bond.\n",
    "\n",
    "A **fixed-coupon bond** is only slightly more complex. The investor receives a cash interest payment each period and the face value (along with the final interest payment) at maturity. The bond's **coupon rate** is a percentage of the face value and determines the amount of the interest payments. *The coupon rate and the yield are two different things*. We only use the coupon rate to determine the coupon payment (PMT). The yield to maturity (I/Y) is the discount rate implied by the bond's price.\n",
    "\n",
    "*The relationship between prices and yields is inverse.* When the price decreases, the yield to maturity increases. When the price increases, the yield to maturity decreases. Or, equivalently, when the yield increases, the price decreases. When the yield decreases, the price increases.\n",
    "\n",
    "An **amortizing bond** is one that pays a level amount each period, including its maturity period. We can determine an annuity payment using a financial calculator.\n",
    "\n",
    "A bond with no maturity date is a **perpetuity**. The present value of a perpetuity is simply the payment divided by the yield (as a decimal).\n",
    "\n",
    "A **preferred stock** is similar to a perpetuity and we calculate its present value the same way:\n",
    "\\\n",
    "$$\n",
    "\\text{preferred stock value } = \\Large\\frac{\\text{dividend per period}}{\\text{the market's required return}}\n",
    "$$\n",
    "\n",
    "**Common stock** typically does not promise a fixed dividend payment. Instead, the company's management decides whether and when to pay common dividends. Because the future cash flows are uncertain, we must use models to estimate a common stock's value.\n",
    "\n",
    "* If we assume a *constant future dividend*, we can value a common stock the same way we value a preferred stock.\n",
    "* If we assume a *constant growth rate of dividends*, we can use a model called the constant growth dividend discount model or the Gordon growth model. We will illustrate this model later in the Equity Investments section of this book.\n",
    "* If we assume a *changing growth rate of dividends*, we must choose an appropriate model. One example (which we will also demonstrate in the Equity Investments section) is a multistage dividend discount model, in which we assume a period of rapid growth\n",
    "followed by a period of constant growth.\n",
    "\n",
    "The **cash flow additivity principle** states that the PV of any stream of cash flows equals the sum of the PVs of the cash flows. If we have two series of cash flows, the sum of the PVs of the two series is the same as the PVs of the two series taken together. We can also divide up a series of cash flows any way we like, and the PV of the \"pieces\" will equal the PV of the original series.\n",
    "\n",
    "The cash flow additivity principle is the basis for the **no-arbitrage principle**, or \"law of one price,\" which says that if two sets of future cash flows are identical under all conditions, they will have the same price today. (If they don't, investors will quickly buy the lower-priced one and sell the higher-priced one, which will drive their prices together). We apply the no-arbitrage condition when we calculate forward exchange rates or forward interest rates, and when we value options using a binomial model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b561d815-df87-498f-9f12-c377a8099b41",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Reading 3: Statistical Measures of Asset Returns\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414fc571-b67d-4f9a-910b-15ca589d78b4",
   "metadata": {},
   "source": [
    "### Measures of Central Tendency\n",
    "\n",
    "The **arithmetic mean** is the sum of the observation values divided by the number of observations. It is the most widely used measure of central tendency. An example of an arithmetic mean is a **sample mean**, which is the sum of all the values in a sample divided by the number of observations in the sample.\n",
    "\n",
    "The **median** is the midpoint of a dataset, where the data are arranged in ascending or descending order. Half of the observations lie above the median, and half are below.\n",
    "\n",
    "The **mode** is the value that occurs most frequently in a dataset. A dataset may have more than one mode, or even no mode. When a distribution has one value that appears most frequently, it is said to be **unimodal**. For continuous data, such as investment returns, we typically do not identify a single outcome as the mode. Instead we divide the relevant range of outcomes into intervals and identify the **modal interval** as the one into which the largest number of observations fall.\n",
    "\n",
    "As mentioned earlier, we can use a <u>*trimmed mean*</u> or a <u>*winsorized mean*</u> to lessen the effects of outliers.\n",
    "\n",
    "### Measures of Location and Dispersion\n",
    "\n",
    "Examples of **quantiles** include the following:\n",
    "\n",
    "* **Quartile**. A distribution is divided into quarters. The difference between the third quartile and the first quartile is known as the interquartile range.\n",
    "* **Quintile**. A distribution is divided into fifths.\n",
    "* **Decile**. A distribution is divided into tenths.\n",
    "* **Percentile**. A distribution is divided into hundredths (percentages). Any quantile can be expressed as a percentile. For example, the third quartile is the 75th percentile.\n",
    "\n",
    "**Dispersion** is variability around the central tendency. In finance and investments, central tendency measures expected return and dispersion measures risk.\n",
    "\n",
    "The **range** of a dataset is the difference between its largest and its smallest value.\n",
    "\n",
    "The **mean absolute deviation (MAD)** is the average of the absolute values of the deviations of individual observations from the arithmetic mean. It uses absolute values because actual deviations from the arithmetic mean always sum to zero.\n",
    "\n",
    "The **sample variance** is the measure of dispersion that applies when we are evaluating a sample of n observations from a population. Sample variance is calculated using the following formula:\n",
    "\n",
    "$$\n",
    "s^2 \\Large = \\frac{\\sum_{i=1}^n \\Big(X_i - \\overline{X} \\Big)^2}{n-1}\n",
    "$$\n",
    "\n",
    "The computed variance is in squared units of measurement. We can address this problem by using the **standard deviation**, which is the square root of the variance. The units of standard deviation are the same as the units of the data:\n",
    "\n",
    "\n",
    "$$\n",
    "s \\Large = \\sqrt{\\frac{\\sum_{i=1}^n \\Big(X_i - \\overline{X} \\Big)^2}{n-1}}\n",
    "$$\n",
    "\\\n",
    "\\\n",
    "**Relative dispersion** is the amount of variability in a distribution around a reference point or benchmark. Relative dispersion is commonly measured with the **coefficient of variation (CV)**, which measures the amount of dispersion in a distribution relative to its mean:\n",
    "\n",
    "$$\n",
    "\\text{CV } = \\Large\\frac{\\text{standard deviation}}{\\text{arithmetic mean}}\n",
    "$$\n",
    "\\\n",
    "\\\n",
    "In an investments setting, the CV is used to measure the risk (variability) per unit of expected return (mean). A lower CV is better.\n",
    "\n",
    "In some situations, it may be more appropriate to consider only **downside risk**, or outcomes less than the mean (or some other specific value). One measure of downside risk is **target downside deviation**, which is also known as **target semideviation**:\n",
    "\n",
    "$$\n",
    "\\Large s_{\\text{target}}  = \\sqrt{\\frac{\\sum_{\\text{all, }X_i-B}^n \\Big(X_i - B \\Big)^2}{n-1}}\n",
    "$$\n",
    "\n",
    "Where $B$ = the target\n",
    "\n",
    "**Skewness**, or skew, refers to the extent to which a distribution is not symmetrical. Nonsymmetrical distributions may be either positively or negatively skewed and result from the occurrence of outliers in the dataset. \n",
    "\\ \n",
    "* *A positively skewed distribution* has outliers greater than the mean (in the upper region, or right tail) and is said to be skewed right. \n",
    "\n",
    "* *A negatively skewed distribution* has outliers less than the mean that fall within its lower (left) tail and is said to be skewed left.\n",
    "\n",
    "Skewness affects the location of the mean, median, and mode:\n",
    "\n",
    "* For a symmetrical distribution, the mean, median, and mode are equal.\n",
    "* For a positively skewed distribution, the mode is less than the median, which is less than the mean.\n",
    "* For a negatively skewed distribution, the mean is less than the median, which is less than the mode.\n",
    "\n",
    "**Kurtosis** is the degree to which a distribution is more or less peaked than a normal distribution. \n",
    "\\\n",
    "**Leptokurtic** describes a distribution that is more peaked than a normal distribution, whereas \n",
    "\\\n",
    "**platykurtic** refers to a distribution that is less peaked, or flatter than a normal one. \n",
    "\\\n",
    "A distribution is **mesokurtic** if it has the same kurtosis as a normal distribution.\n",
    "\n",
    "A distribution is said to exhibit **excess kurtosis** if it has either more or less kurtosis than the normal distribution. The computed kurtosis for all normal distributions is three. Statisticians, however, sometimes report excess kurtosis, which is defined as kurtosis minus three. Thus, a normal distribution has excess kurtosis equal to zero, a leptokurtic distribution has excess kurtosis greater than zero, and platykurtic distributions have excess kurtosis less than zero.\n",
    "\n",
    "Securities returns tend to exhibit both skewness and kurtosis. If returns are modeled using an assumed normal distribution, the predictions will not take into account the potential for extremely large, negative outcomes. Most risk managers put little emphasis on the mean and standard deviation and focus more on returns in the tails of the distribution—that is where the risk is.\n",
    "\n",
    "**Covariance** measures how two variables move together. Sample covariance is calculated as follows:\n",
    "\n",
    "$$\n",
    "S_{XY} \\Large = \\frac{\\sum_{i=1}^n \\Big(\\Big[ X_i - \\overline{X} \\Big] \\Big[Y_i - \\overline{Y}\\Big] \\Big) }{n-1}\n",
    "$$\n",
    "\n",
    "where: \n",
    "\n",
    "$X_i$ = an observation of variable X\n",
    "\n",
    "$Y_i$ = an observation of variable Y\n",
    "\n",
    "$\\overline{X}$ = mean of variable X\n",
    "\n",
    "$\\overline{Y}$ = mean of variable Y\n",
    "\n",
    "$n$ = number of periods\n",
    "\n",
    "Like the variance, the units of covariance are the square of the units used for the data. The value of covariance depends on the units of the variables, and we cannot use it to interpret the relative strength of the relationship. To address these issues, we use a standardized measure of the linear relationship between two variables called the **correlation coefficient**. The correlation between two variables, $X$ and $Y$, is:\n",
    "\n",
    "$$\n",
    "\\Large\\rho_{XY} = \\frac{S_{\\text{XY}}}{S_X S_Y}\n",
    "$$\n",
    "\n",
    "Correlation has no units. Its values range from `–1 to +1`. If correlation is `+1` (perfect positive correlation), a change in one random variable results in a proportional change in the other. If correlation is `–1` (perfect negative correlation), a change in one random variable results in a proportional but opposite change in the other. \n",
    "\\\n",
    "If correlation is zero, the two variables have no linear relationship.\n",
    "\\\n",
    "\\\n",
    "Two variables might have a *nonlinear relationship* that the correlation coefficient does not reflect. Analysts often use **scatter plots**, with one variable on the vertical axis and the other on the horizontal axis, to reveal nonlinear relationships. Variables can exhibit **spurious correlation** that results from chance or from their association with a third variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb0715-2a43-4c65-97db-2ed68cd6cd92",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Reading 4: Probability Trees and Conditional Expectations\n",
    "\n",
    "The **expected value** of a random variable is the weighted average of the possible outcomes for the variable:\n",
    "\n",
    "$$\n",
    "E(X) = ΣP(xi)xi = P(x1)x1 + P(x2)x2+ … + P(xn)xn\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "P(xi) = probability of outcome xi\n",
    "\n",
    "**Variance (from a probability model)** can be calculated as the probability-weighted sum of the squared deviations from the expected value. The standard deviation is the positive square root of the variance.\n",
    "\n",
    "When we estimated standard deviation from sample data, we divided the sum of the squared deviations from the mean by the sample size minus one. With a probability model, by contrast, we have no past observations (and therefore no \"n – 1\" to divide by). We use probability weights instead, as they describe the entire distribution of outcomes.\n",
    "\n",
    "A general framework called a **probability tree** is used to show the probabilities of various outcomes. The following  **A Probability Tree for an Investment Problem** shows an example.\n",
    "\n",
    "##### A Probability Tree for an Investment Problem\n",
    "\n",
    "<img src=\"https://github.com/PachaTech/CFA-Level-1/blob/main/Secret%20Sauce/Quants/pics/x1_2_8_3_2.jpeg?raw=true\">\n",
    "\n",
    "**Conditional expected values** depend on the outcome of some other event. An analyst can use a conditional expected value to revise his expectations when new information arrives.\n",
    "\n",
    "**Bayes' formula** is used to update a given set of prior probabilities for a given event in response to the arrival of new information. The rule for updating prior probability of an event is as follows:\n",
    "\n",
    "$$\n",
    "\\text{updated probability } = \\Large \\frac{\\text{probability of new information for a given event}}{\\text{unconditional probability of new information}} \\small \\times \\text{ prior probability of event}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1743d33-1dcf-43a7-87cf-8a435828f71e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Reading 5: Portfolio Mathematics\n",
    "\n",
    "<hr>\n",
    "\n",
    "The **expected return and variance for a portfolio** of assets can be determined using the expected returns and weights of the individual assets in the portfolio:\n",
    "\n",
    "$$\n",
    "\\Large\\mathbb{E}(\\textrm{R}_\\textrm{p}) = w_A R_A + w_B R_B\n",
    "$$\n",
    "\\\n",
    "$$\n",
    "\\Large\\textrm{Var}_\\textrm{P}  = \\Large w^2_A \\sigma^2_A + w^2_B \\sigma^2_B + 2w_A w_B \\sigma_A \\sigma_B \\rho_{\\textrm{A,B}}\n",
    "$$\n",
    "or:\n",
    "$$\n",
    "\\Large\\textrm{Var}_\\textrm{P} =  \\Large w^2_A \\sigma^2_A + w^2_B \\sigma^2_B + 2w_A w_B \\textrm{Cov}_{\\textrm{A,B}}\n",
    "$$\n",
    "\\\n",
    "We can calculate **covariance of returns** two assets in a portfolio as follows:\n",
    "$$\n",
    "\\Large \\textrm{Cov}_{\\textrm{A,B}} = \\mathbb{E} \\Big( \\big[R_A - E(R_A) \\big] \\big[ R_B − E(R_B) \\big] \\Big)\n",
    "$$\n",
    "\\\n",
    "**Shortfall risk** is the probability that a portfolio's return or value will be below a specified target return or value over a specified period. **Roy's safety-first criterion** states that the optimal portfolio minimizes the probability that the return of the portfolio falls below some minimum acceptable \"threshold\" level.\n",
    "\n",
    "\n",
    "Roy's **safety-first ratio (SFRatio)** shows how many standard deviations the expected return is above the threshold return $(R_L)$:\n",
    "$$\n",
    "\\textrm{SFRatio}= \\Large\\frac{\\textrm{E}(R_p)−R_L}{\\sigma_p}\n",
    "$$\n",
    "\\\n",
    "A portfolio with a higher SFRatio is preferred. The greater the SFRatio, the lower the probability that returns will be below the threshold return, and therefore the lower the shortfall risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2053e69-5492-4856-8599-734c5569ba37",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Reading 6: Simulation Methods\n",
    "\n",
    "<hr>\n",
    "\n",
    "If $\\textrm{x}$ is normally distributed, $\\textrm{Y} = e^{\\textrm{x}}$ is lognormally distributed. The **lognormal distribution** is positively skewed as shown in the following  **Lognormal Distribution**.\n",
    "\n",
    "*Lognormal Distribution*\n",
    "\n",
    "<img src=\"https://github.com/PachaTech/CFA-Level-1/blob/main/Secret%20Sauce/Quants/pics/x1_3_9_14.jpeg?raw=true\">\n",
    "\n",
    "The lognormal distribution is useful for modeling asset prices if we think of an asset's future price as the result of a continuously compounded return on its current price. That is:\n",
    "\n",
    "$$\n",
    "\\Large\\textrm{P}_\\textrm{T} = \\textrm{P}_0 e^{\\textrm{r}_{0,\\textrm{T}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\textrm{P}_\\textrm{T}$ = asset price at time $\\textrm{T}$\n",
    "\n",
    "$\\textrm{P}_0$ = asset price at time $0$ (today)\n",
    "\n",
    "$\\textrm{r}_{0,\\textrm{T}}$ = continuously compounded return on the asset from time $0$ to time $\\textrm{T}$\n",
    "\n",
    "<br>\n",
    "\n",
    "In many pricing models, we assume returns are independently and identically distributed.\n",
    "\n",
    "* If returns are *independently* distributed, past returns are not useful for predicting future returns.\n",
    "* If returns are *identically* distributed, they exhibit \"stationarity,\" in that their mean and variance do not change over time.\n",
    "\n",
    "**Monte Carlo simulation** is performed by making assumptions about the distributions of prices or risk factors and using a large number of computer-generated random values for the relevant risk factors or prices to generate a distribution of possible outcomes (e.g., project NPVs, portfolio values). An advantage of Monte Carlo simulation is that its inputs are not limited to the range of historical data. Its limitations are that it is fairly complex, can provide answers that are no better than the assumptions used, and is a statistical method that cannot offer the insights provided by an analytic method.\n",
    "\n",
    "**Resampling** is another method for generating data inputs to use in a simulation. To conduct resampling, we start with the observed sample and repeatedly draw subsamples from it, each with the same number of observations. From these samples, we can infer parameters for the population, such as its mean and variance.\n",
    "\n",
    "In **bootstrap resampling**, we draw repeated samples from the full dataset, replacing the sampled observations each time so that they might be redrawn in another sample. We can then directly calculate the standard deviation of these sample means as our estimate of the standard error of the sample mean. Simulation using data from bootstrap resampling follows the same procedure as Monte Carlo simulation. The difference is the source and scope of the data. For example, if a simulation uses bootstrap resampling of historical returns data, its inputs are limited by the distribution of actual outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee928d-254a-4b85-aa80-373e9a6c98dc",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Reading 7: Estimation and Inference\n",
    "\n",
    "<hr>\n",
    "\n",
    "Probability sampling refers to selecting a sample when we know the probability of each sample member in the overall population.\n",
    "\n",
    "* With **simple random sampling**, each item or person in the population has the same likelihood of being included in the sample.\n",
    "* Another way to form an approximately random sample is **systematic sampling**, selecting every $n$th member from a population.\n",
    "* **Stratified random sampling** separates a population into groups based on one or more characteristics, then takes a random sample from each group based on its size. For example, in constructing bond index portfolios, we may first divide the bonds into groups by maturity, rating, and embedded options, then pick bonds from each group in proportion to the number of index bonds in that group. This ensures that our \"random\" sample has similar maturity, rating, and call characteristics to the index.\n",
    "* **Cluster sampling** is also based on subsets of a population, but assumes that each subset (cluster) is representative of the overall population. In **one-stage cluster sampling**, a random sample of clusters is selected and all the data in those clusters comprise the sample. In **two-stage cluster sampling**, random samples from each of the selected clusters comprise the sample. To the extent that the subgroups do not have the same distribution as the entire population of the characteristic we are interested in, cluster sampling will have greater sampling error than simple random sampling and two-stage cluster sampling can be expected to have greater sampling error than one-stage cluster sampling.\n",
    "\n",
    "**Non-probability sampling** is based on either low cost or easy access to some data items, or on using the judgment of the researcher in selecting specific data items. Less randomness in selection may lead to greater sampling error.\n",
    "\n",
    "* **Convenience sampling** refers to selecting sample data based on its ease of access, using data that are readily available.\n",
    "* **Judgmental sampling** refers to samples for which each observation is selected from a larger data set by the researcher, based on her experience and judgment.\n",
    "\n",
    "\n",
    "#### Central Limit Theorem\n",
    "\n",
    "The **central limit theorem** of statistics states that in selecting simple random samples of size n from a population with a mean µ and a finite variance $\\sigma^2$, the sampling distribution of the sample mean approaches a normal probability distribution with mean $\\mu$ and a variance equal to $\\sigma^2/n$ as the sample size becomes sufficiently large $(n ≥ 30)$.\n",
    "\n",
    "The **standard error of the sample mean** is the standard deviation of the sample divided by the square root of $n$, the number of observations in the sample. As the sample size increases, the sample mean gets closer, on average, to the true mean of the population.\n",
    "\n",
    "There are alternatives to calculating the standard error of the sample mean:\n",
    "\n",
    "* With the **jackknife method**, we calculate multiple sample means, each with one of the observations removed from the sample. The standard deviation of these sample means can then be used as an estimate of the standard error of sample means.\n",
    "* With the more computationally demanding **bootstrap method**, we draw repeated samples of size $n$ from the full data set (replacing the sampled observations each time) and then calculate the standard deviation of these means. The bootstrap method can be used to estimate the distributions of complex statistics, including those that do not have an analytic form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5e84f-4201-41ab-821b-9e414c185c25",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Reading 8: Hypothesis Testing\n",
    "\n",
    "<hr>\n",
    "\n",
    "A **hypothesis** is a statement about a population parameter that is to be tested. For example, \"The mean return on the S&P 500 Index is equal to zero.\"\n",
    "\n",
    "#### Steps in Hypothesis Testing\n",
    "\n",
    "* State the hypothesis.\n",
    "* Select a test statistic.\n",
    "* Specify the level of significance.\n",
    "* State the decision rule for the hypothesis.\n",
    "* Collect the sample and calculate statistics.\n",
    "* Make a decision about the hypothesis.\n",
    "* Make a decision based on the test results.\n",
    "* Null and Alternative Hypotheses\n",
    "\n",
    "The **null hypothesis**, designated as $H_0$, is the hypothesis the researcher wants to reject. It is the hypothesis that is actually tested and is the basis for the selection of the test statistics. Thus, if you seek to show that the mean return on the S&P 500 Index is different from zero, the null hypothesis will be that the mean return on the index equals zero.\n",
    "\n",
    "The **alternative hypothesis**, designated $H_a$, is what is concluded if there is sufficient evidence to reject the null hypothesis. It is usually the alternative hypothesis you are really trying to support. Why? Since you can never really prove anything with statistics, when the null hypothesis is rejected, the implication is that the (mutually exclusive) alternative hypothesis is valid.\n",
    "\n",
    "A **test statistic** is calculated from sample data and is compared to a critical value to evaluate $H_0$. The most common test statistics are the z-statistic and the t-statistic. Critical values come from tables and are based on the researcher's desired level of significance. If the test statistic exceeds the critical value (or is outside the range of critical values), the researcher rejects $H_0$. As the level of significance gets smaller, the critical value gets larger and it becomes more difficult to reject the null hypothesis. The **p-value** of a test statistic is the lowest significance value that will result in rejection of the null hypothesis.\n",
    "\n",
    "#### Type I and Type II Errors\n",
    "\n",
    "When testing a hypothesis, there are two possible types of errors:\n",
    "\n",
    "* **Type I error**. Rejection of the null hypothesis when it is actually true.\n",
    "* **Type II error**. Failure to reject the null hypothesis when it is actually false.\n",
    "\n",
    "The power of a test is `1 – P(Type II error)`. The more likely that a test will reject a false null, the more powerful the test. A test that is unlikely to reject a false null hypothesis has little power.\n",
    "\n",
    "The significance level is the probability of making a Type I error (rejecting the null when it is true) and is designated by the Greek letter alpha ($\\alpha$). You can think of this as the probability that the test statistic will exceed or fall below the critical values by chance even though the null hypothesis is true. A significance level of 5% ($\\alpha$ = 0.05) means there is a 5% chance of rejecting a true null hypothesis.\n",
    "\n",
    "<img src=\"https://github.com/PachaTech/CFA-Level-1/blob/main/Secret%20Sauce/Quants/pics/x1.png?raw=true\">\n",
    "\n",
    "#### Types of Hypothesis Tests\n",
    "\n",
    "* For a hypothesis concerning the *value of a population mean*, we use a t-test (or a *z-test* if the sample size is large enough).\n",
    "* To test a hypothesis concerning the *equality of two population means*, we use a *t-test*. The nature of that test depends on whether the samples are independent (a difference in means test) or dependent (a paired comparisons test).\n",
    "* For a hypothesis concerning the *value of a population variance*, we use a chi-square test.\n",
    "* To test a hypothesis concerning the *equality of two population variances*, we use an *F-test*.\n",
    "\n",
    "\n",
    "The following table summarizes the test statistics used for each type of hypothesis test.\n",
    "\n",
    "<img src=\"https://github.com/PachaTech/CFA-Level-1/blob/main/Secret%20Sauce/Quants/pics/x1_2.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c554151-b58b-4ede-acae-0394cf0d58d8",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Reading 9: Parametric and Non-Parametric Tests of Independence\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Parametric tests**, like the *t*-test, *F*-test, and chi-square test, make assumptions regarding the distribution of the population from which samples are drawn. **Non-parametric tests** either do not consider a particular population parameter or have few assumptions about the sampled population.\n",
    "\n",
    "A parametric test of whether two random variables are independent uses a hypothesis that their correlation coefficient equals zero. Its test statistic follows a $t$-distribution with `n – 2` degrees of freedom and increases with the sample size as well as with the sample correlation coefficient:\n",
    "\n",
    "$$\n",
    "\\Large \\frac{r \\sqrt{n-2}}{\\sqrt{1-r^2}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$r$ = sample correlation\n",
    "\n",
    "$n$ = sample size\n",
    "\n",
    "\n",
    "The **Spearman rank correlation test**, a nonparametric test, can be used to test whether two sets of ranks are correlated. Ranks are simply ordered values.\n",
    "\n",
    "A **contingency table** is a two-dimensional array with rows that represent attributes of one of the variables and with columns that represent attributes of the other variable. The values in each cell are the frequencies with which we observe two attributes simultaneously. A hypothesis test of whether two characteristics of a sample are independent uses a Chi-square statistic calculated from a contingency table. The test compares the actual table values to what the values would be if the two characteristics were independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecd2b4-9f6d-4720-a8bf-635026e92461",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<hr>\n",
    "\n",
    "# Reading 10: Simple Linear Regression\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Simple linear regression** is used to estimate the linear relationship between two variables and evaluate the significance of the relationship. A researcher determines which variable likely explains the variation in the other.\n",
    "\n",
    "The **dependent variable** (the $Y$ variable) is also referred to as the explained variable, the endogenous variable, or the predicted variable.\n",
    "\n",
    "The **independent variable** (the $X$ variable) is also referred to as the explanatory variable, the exogenous variable, or the predicting variable.\n",
    "\n",
    "The following **linear regression model** is used to describe the relationship between variables $X$ (independent) and $Y$ (dependent):\n",
    "\n",
    "$$\n",
    "\\Large Y_i = b_0 + b_1 X_i + \\epsilon_i \\textrm{ , with }  i = 1 \\textrm{  to  } n\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$Y_i$ = $i$th observation of the dependent variable, $Y$\n",
    "\n",
    "$X_i$ = $i$th observation of the independent variable, $X$\n",
    "\n",
    "$b_0$ = regression intercept term\n",
    "\n",
    "$b_1$ = regression slope coefficient\n",
    "\n",
    "$ε_i$ = **residual** for the $i$th observation (also referred to as the error term)\n",
    "\n",
    "<p><br></p>\n",
    "\n",
    "Based on this regression model, the regression process estimates an equation for a line through a scatter plot of the data that best explains the observed values for $Y$ in terms of the observed values for $X$. This **regression line** takes the following form:\n",
    "\n",
    "\n",
    "$$\n",
    "\\Large \\hat{Y_i} = \\hat{b_0} + \\hat{b_1} X_i \\textrm{ , for  i } = 1 \\textrm{  to  } \\text{n}\n",
    "$$\n",
    "\n",
    "**where:**\n",
    "\n",
    "$\\hat{Y_i}$ = estimated value of $Y_i$ given $X_i$\n",
    "\n",
    "$\\hat{b_0}$ = estimated intercept term\n",
    "\n",
    "$\\hat{b_i}$ = estimated **slope coefficient**\n",
    "\n",
    "<p><br></p>\n",
    "\n",
    "The regression line is the line through the scatter plot of $X$ and $Y$ that minimizes the sum of the squared errors (differences between the $Y$-values predicted by the regression equation $\\hat{Y_i}$​ and the observed Y-values $(Y_i)$). The sum of these squared differences is called the **sum of squared errors (SSE)**.\n",
    "\n",
    "The slope coefficient for the regression line is an estimate of the change in $Y$ for a one-unit change in $X$. The slope term is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\Large\\hat{b_1} = \\frac{\\textrm{COV}_{\\text{XY}}} {\\sigma^2 \\text{X}}\n",
    "$$\n",
    "\n",
    "The **intercept term** is the regression line's intersection with the $Y$-axis at X = 0. The intercept term may be expressed as follows\n",
    "$$\n",
    "\\Large \\hat{b_1} = \\overline{Y} - \\hat{b_1} \\overline{X}\n",
    "$$\n",
    "\n",
    "In other words, the regression line passes through a point with coordinates equal to the means of the independent and dependent variables.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Assumptions of Simple Linear Regression\n",
    "\n",
    "Linear regression assumes the following:\n",
    "\n",
    "1. A linear relationship exists between the dependent and the independent variables.\n",
    "2. The variance of the residual term is constant for all observations (homoskedasticity). **Heteroskedasticity** refers to the situation when this assumption is violated.\n",
    "3. The residual term is independently distributed; that is, the residual for one observation is not correlated with that of another observation.\n",
    "4. The residual term is normally distributed.\n",
    "\n",
    "#### Analysis of Variance (ANOVA)\n",
    "\n",
    "ANOVA is a statistical procedure for analyzing the total variation in the dependent variable. Three of the measures in an ANOVA table are as follows:\n",
    "\n",
    "* **Sum of squares total (SST)**. Sum of the squared differences between the actual $Y$-values and the mean of $Y$. SST measures the total variation in the dependent variable.\n",
    "* **Sum of squares regression (SSR)**. Sum of the squared differences between the predicted $Y$-values and the mean of $Y$. SSR measures the variation in the dependent variable that *is* explained by the independent variable.\n",
    "* **Sum of squared errors (SSE)**. Sum of the squared differences between the actual $Y$-values and the predicted $Y$-values. SSE measures the variation in the dependent variable that *is not* explained by the dependent variable.\n",
    "\n",
    "\n",
    "From these, we can calculate the **coefficient of determination**, or **R-squared**, which is the proportion of the total variation in the dependent variable that is explained by the independent variable:\n",
    "\n",
    "$$\n",
    "\\Large \\textrm{R}^2 = \\textrm{SSR} / \\textrm{SST}\n",
    "$$\n",
    "\n",
    "The **.mean square regression (MSR)**. is equal to the SSR divided by the regression's degrees of freedom. For a simple linear regression the degrees of freedom are one, because we have one independent variable.\n",
    "\n",
    "The **.mean squared error (MSE)**. is the SSE divided by its degrees of freedom, which for a simple linear regression is the number of observations minus two.\n",
    "\n",
    "$$\n",
    "\\Large \\textrm{MSE} = \\frac {\\textrm{SSE}} {n-2}\n",
    "$$\n",
    "\n",
    "The **standard error of estimate (SEE)** for a regression is the standard deviation of its residuals. The lower the SEE, the better the model fit.\n",
    "\n",
    "$$\n",
    "\\Large \\textrm{SEE} = \\sqrt{\\textrm{MSE}}\n",
    "$$\n",
    "\n",
    "Dividing the MSR by the MSE gives us an $F$-statistic that we can use to test whether the slope coefficient is statistically significant.\n",
    "\n",
    "$$\n",
    "\\Large \\textrm{F} = \\textrm{MSR} / \\textrm{MSE} \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Hypothesis Test of the Slope Coefficient\n",
    "\n",
    "For a simple linear regression, the $F$-test is equivalent to a t-test of the significance of the estimated slope coefficient ($b_1$). The test statistic is as follows:\n",
    "$$\n",
    "\\Large t = \\frac{\\hat{b_1}-b}{S_{\\hat{b_1}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$b_1$ = hypothesized value of slope coefficient\n",
    "\n",
    "$S_{\\hat{b_1}}$ = standard error of slope coefficient\n",
    "or\n",
    "$$\n",
    "\\Large \\hat{b_1} = \\frac{\\textrm{SEE}}{\\sqrt{\\sum_{i=1}^n \\big(X_i - \\overline{X} \\big)^2}}\n",
    "$$\n",
    "\n",
    "Degrees of freedom for this $t$-test are `n – 2`. The null hypothesis is that $b_1 = 0$ (the independent variable has no significant explanatory power for the value of the dependent variable).\n",
    "\n",
    "#### Predicted Values and Confidence Intervals\n",
    "\n",
    "We use a regression model to predict values of the dependent variable, given predicted values for the independent variable. For example, given the following regression equation:\n",
    "$$\n",
    "\\large \\hat{Y} = -2.3\\% + 0.64 \\hat{X}\n",
    "$$\n",
    "If we have a forecast value for $X$ of `10%`, the predicted value of $Y$ is `–2.3% + 0.64(10%) = 4.1%`.\n",
    "\n",
    "We can construct a confidence interval around the predicted value, as follows:\n",
    "\n",
    "$\\qquad Y \\pm \\big(t_c \\times S_f \\big)$\n",
    "\\\n",
    "where:\n",
    "\\\n",
    "$t_c$ = two-tailed critical $t$-value at the desired level of significance with `df = n – 2`\n",
    "\\\n",
    "$S_f$ = standard error of the forecast \n",
    "$$\n",
    "\\Large S_f= \\text{SEE}^2 \\Biggr[ 1 + \\frac{1}{n} + \\frac{(\\text{X}-\\text{X})^2}{(n-1)^2_\\text{X}}    \\Biggr]\n",
    "$$\n",
    "\n",
    "On the Level I exam, we believe any question that requires the standard error of the forecast is highly likely to provide a value for it.\n",
    "\n",
    "#### Linear Regression with Transformed Variables\n",
    "\n",
    "If one or both of the variables appear to be growing at a constant rate in percentage terms (constant compound, or exponential, rate of growth), we can transform them using the natural logarithm and then apply simple linear regression. Such a model is called a **log-lin model** if the dependent variable is transformed while the independent variable is linear, a **lin-log model** if the independent variable is transformed while the dependent variable is linear, or a **log-log model** if both variables are transformed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d28a0-4102-4da8-b3e7-7ae7d2b1a05f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Reading 11: Introduction to Big Data Techniques\n",
    "\n",
    "<hr>\n",
    "\n",
    "The term **fintech** refers to developments in technology that can be applied to the financial services industry. Examples include tools and techniques for analyzing very large datasets, such as artificial intelligence.\n",
    "\n",
    "**Big Data** is an expression that refers to all the potentially useful information that is generated in the economy, from traditional sources (financial markets, company financial reports, and economic statistics) as well as **alternative data** from nontraditional sources (data from individuals' online activity, **corporate exhaust** such as bank records and retail scanner data, and data from the **Internet of Things** such as smartphones and smart buildings).\n",
    "\n",
    "Characteristics of Big Data include its volume, velocity, and variety. The *volume* of data continues to grow by orders of magnitude and is now measured in terabytes (1,000 gigabytes) and even petabytes (1,000 terabytes). Velocity refers to how quickly data are communicated. Real-time data are said to have low **latency** while data communicated with a lag have high latency. The variety of data ranges from structured forms such as spreadsheets and databases, to semistructured forms such as photos and web page code, to unstructured forms such as video.\n",
    "\n",
    "**Data science** describes methods for processing and visualizing data. Processing methods include *capture* (collecting data), *curation* (assuring data quality), *storage* (archiving and accessing data), *search* (finding needed information in stored data), and *transfer* (moving data to where they are needed). Visualization techniques include charts and graphs for structured data, and tools such as word clouds or mind maps for less structured data.\n",
    "\n",
    "**Artificial intelligence**, or computer systems that can be programmed to simulate human cognition, may be applied to organize unstructured data. **Neural networks** are an example of artificial intelligence. In **machine learning**, a computer algorithm is given inputs of source data and designed to learn how to recognize patterns in the input data or model output data.\n",
    "\n",
    "In **supervised learning**, the input and output data are labeled, the machine learns to model the outputs from the inputs, and then the machine is given new data on which to use the model. In **unsupervised learning**, the input data are not labeled, and the machine learns to describe the structure of the data. **Deep learning** uses layers of neural networks to identify patterns, and may employ supervised or unsupervised learning. Image and speech recognition are among the applications of deep learning.\n",
    "\n",
    "**Overfitting** occurs when the machine creates a model that is too complex: the machine learns the input and output data too exactly and identifies spurious patterns and relationships. **Underfitting** occurs when the model is not complex enough to describe the data: the machine fails to identify actual patterns and relationships.\n",
    "\n",
    "Applications of fintech that are relevant to investment management include text analytics, natural language processing, risk governance, and algorithmic trading.\n",
    "\n",
    "* **Text analytics** refers to the analysis of unstructured data in text or voice forms. It has the potential to partially automate tasks such as evaluating company regulatory filings.\n",
    "* **Natural language processing** refers to the use of computers and artificial intelligence to interpret human language. Possible applications in finance could be to check for regulatory compliance in employee communications, or to evaluate large volumes of research reports to detect subtle changes in sentiment.\n",
    "* **Risk governance** requires an understanding of a firm's exposure to a wide variety of risks. Machine learning and other techniques related to Big Data can be useful in modeling and testing risk, particularly if firms use real-time data to monitor risk exposures.\n",
    "* **Algorithmic trading** is computerized securities trading based on predetermined rules. This can be useful for optimizing execution instructions, dividing large orders across exchanges in the best way, or to carry out **high-frequency trading**| that takes advantage of intraday securities mispricings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
