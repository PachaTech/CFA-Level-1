{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54050dc2",
   "metadata": {},
   "source": [
    "# Module 10.1: Linear Regression Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7fb921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.core.common import random_state\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7ab11",
   "metadata": {},
   "source": [
    "### LOS 10.a: Describe a simple linear regression model, how the least squares criterion is used to estimate regression coefficients, and the interpretation of these coefficients.\n",
    "\n",
    "The purpose of simple linear regression is to explain the variation in a dependent variable in terms of the variation in a single independent variable. Here, the term variation is interpreted as the degree to which a variable differs from its mean value. Don't confuse variation with variance—they are related, but they are not the same.\n",
    "\n",
    "\n",
    "$\\text{variation in Y} = {\\Large\\sum_{1}^{n}} (Y_i - \\bar{Y})^2$\n",
    "\n",
    "* The **dependent variable** is the variable whose variation is explained by the independent variable. We are interested in answering the question, \"What explains fluctuations in the dependent variable?\" The dependent variable is also referred to as the terms explained variable, endogenous variable, or predicted variable.\n",
    "</br>\n",
    "\n",
    "* The **independent variable** is the variable used to explain the variation of the dependent variable. The independent variable is also referred to as the terms explanatory variable, exogenous variable, or predicting variable.\n",
    "</br>\n",
    "\n",
    "**Example: Dependent vs. independent variables**\n",
    "\n",
    "Suppose you want to predict stock returns with GDP growth. Which variable is the independent variable?\n",
    "\n",
    "&emsp;**Answer:**\n",
    "\n",
    "Because GDP is going to be used as a predictor of stock returns, stock returns are being *explained* by GDP. Hence, stock returns are the dependent (explained) variable, and GDP is the independent (explanatory) variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b41f00",
   "metadata": {},
   "source": [
    "#### Simple Linear Regression Model\n",
    "\n",
    "The following linear regression model is used to describe the relationship between two variables, $X$ and $Y$:\n",
    "<br></br>\n",
    "$\\Large{Y_i = b_0 + b_1X_i + \\epsilon_i ,... i = 1, ..., n}$\n",
    "\n",
    "&emsp; \n",
    "\n",
    "<U>where:</U>\n",
    "\n",
    "$Y_i$ = ith observation of the dependent variable, $Y$ \n",
    "\n",
    "$X_i$ = ith observation of the independent variable, $X$\n",
    " \n",
    "$b_0$ = regression intercept term\n",
    " \n",
    "$b_1$ = regression slope coefficient\n",
    " \n",
    "$\\epsilon_i$ = **residual** for the $i_{th}$ observation (also referred to as the disturbance term or error term);\n",
    "\n",
    "Based on this regression model, the regression process estimates an equation for a line through a scatter plot of the data that \"best\" explains the observed values for $Y$ in terms of the observed values for $X$.\n",
    "\n",
    "\n",
    "#### Simple Linear Regression Model\n",
    "\n",
    "The linear equation, often called the line of best fit or regression line, takes the following form:\n",
    "<br></br>\n",
    "\n",
    "$\\Large\\hat{Y}_{i} = \\hat{b}_{0} + \\hat{b}_{1}X_i i=1,2,3...,n$\n",
    "\n",
    "&emsp; \n",
    "\n",
    "<U>where:</U>\n",
    "\n",
    "$\\hat{Y}_{i}$ = estimated value of $Y_i$ given $X_i$\n",
    "\n",
    "$\\hat{b}_{0}$ = estimated intercept term.\n",
    "\n",
    "$\\hat{b}_{1}$ = estimated slope coefficient.\n",
    "\n",
    " \n",
    "<br>\n",
    "The hat \"^\" above a variable or parameter indicates a predicted value.\n",
    "</br>\n",
    "\n",
    "\n",
    "Thus, the regression line is the line that minimizes the **SSE**. This explains why simple linear regression is frequently referred to as ordinary least squares **(OLS) regression**, and the values determined by the estimated regression equation, $\\hat{Y}_i$, are called least squares estimates.\n",
    "\n",
    "<br>\n",
    "The estimated slope coefficient $\\hat{b}_{1}$ for the regression line describes the change in $Y$ for a one-unit change in $X$. It can be positive, negative, or zero, depending on the relationship between the regression variables. The slope term is calculated as follows:\n",
    "</br>\n",
    "&emsp; \n",
    "\n",
    "$\\Large\\hat{b}_{1} = \\frac{CovXY}{\\sigma^2_X}$\n",
    "\n",
    "The intercept term $\\hat{b}_{0}$ is the line's intersection with the $Y$-axis at $X = 0$. It can be positive, negative, or zero. A property of the least squares method is that the intercept term may be expressed as follows:\n",
    "\n",
    "&emsp; \n",
    "$\\large\\hat{b}_{0}=\\bar{Y}−\\hat{b}_{1}\\bar{X}$\n",
    "\n",
    "where:\n",
    "\n",
    "Y = mean of Y\n",
    "\n",
    "X = mean of X\n",
    "\n",
    "The intercept equation highlights the fact that the regression line passes through a point with coordinates equal to the mean of the independent and dependent variables (i.e., the point X, Y).\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Example: Computing the slope coefficient and intercept term**\n",
    "\n",
    "Compute the slope coefficient and intercept term using the following information:\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th></th>\n",
    "    <th></th>\n",
    "    <th></th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Cov(S&amp;P 500, ABC</td>\n",
    "    <td>0.000336</td>\n",
    "    <td>Mean return, S&amp;P 500</td>\n",
    "    <td>−2.70%</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Var(S&amp;P 500)</td>\n",
    "    <td>0.000522</td>\n",
    "    <td>Mean return, ABC</td>\n",
    "    <td>−4.05%</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The slope coefficient is calculated as $\\hat{b}_{1} = \\frac{0.000336}{0.000522} = 0.64$.\n",
    "\n",
    "The intercept term is calculated as follows:\n",
    "\n",
    "$\\hat{b}_{0}=\\overline{ABC}−\\hat{b}_{1}\\overline{SP500}=−4.05\\% −0.64 (−2.70\\%) = −2.3\\%$\n",
    "<br>\n",
    "</br>\n",
    "The estimated regression line that minimizes the SSE in our ABC stock return example is shown in  Estimated Regression Equation for ABC vs. S&P 500 Excess Returns.\n",
    "\n",
    "<br>\n",
    "This regression line has an intercept of $–2.3\\%$ and a slope of $0.64$. The model predicts that if the S&P 500 excess return is $–7.8\\%$ (May 20X4 value), then the ABC excess return would be $–2.3\\% + (0.64)(–7.8\\%) = –7.3\\%$. The residual (error) for the May 20X4 ABC prediction is $8.4\\%$—the difference between the actual ABC excess return of $1.1\\%$ and the predicted return of $–7.3\\%$.\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d09d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "covAB= 0.000336\n",
    "ABC_Var = 0.000522\n",
    "SP500_Mu = -2.70\n",
    "ABC_Mu = -4.05\n",
    "## Define the Slope intercept\n",
    "bhat_1 = covAB / ABC_Var\n",
    "bhat_0 = ABC_Mu - (bhat_1*SP500_Mu)\n",
    "\n",
    "#Excess returns\n",
    "excessSP = bhat_0 + bhat_1 * -7.8\n",
    "\n",
    "#Actual ABC returns\n",
    "actABC = 1.1\n",
    "sse = actABC - excessSP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7439d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope Coeffiecient b^1   =   0.64\n",
      "Intercept b^0            =  -2.3\n",
      "Excess returns of SP500  =  -7.3\n",
      "Actual returns of ABC    =   1.1\n",
      "SSE Error ABC prediction =   8.4\n"
     ]
    }
   ],
   "source": [
    "print(\"Slope Coeffiecient b^1   =  \", round(bhat_1, 2))\n",
    "print(\"Intercept b^0            = \", round(bhat_0, 1))\n",
    "print(\"Excess returns of SP500  = \", round(excessSP, 1))\n",
    "print(\"Actual returns of ABC    =  \", round(actABC, 1))\n",
    "print(\"SSE Error ABC prediction =  \", round(sse, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e268d8",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/PachaTech/CFA-Level-1/blob/main/Module%2010/pics/10_1%20graph.jpeg?raw=true\">\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe3011",
   "metadata": {},
   "source": [
    "A Simple linear regression explains the variation in a dependent variable $Y$ in terms of the variation in a single independent variable $X$\n",
    "\n",
    "#### Assumptions of a Simple Linear Regression\n",
    "\n",
    "1. The relationship between $X$ independent and $Y$ dependent does (must) exist. \n",
    "2. Error terms are normally distrubuted. Their $\\mu=0$\n",
    "3. The variance $\\sigma$ of the error term is constant *(Homoskedastic)*.\n",
    "4. Error terms are independently distributed and uncorrelated with each other, *(serial or autocorrelation)*.  \n",
    "5. Error terms are not random.\n",
    "\n",
    "<hr></hr>\n",
    "\n",
    "#### Results can be an issue for Standard Error terms or to Hypothesis testing.\n",
    "* HOMOSKEDACITY - refers to the case where all prediction errors all have the same constant variance.  $\\sigma = c$\n",
    "<br>\n",
    "\n",
    "* HETEROSKEDACITY - refers to the variance of the error terms $\\epsilon$ not being constant.    $\\sigma \\neq c$\n",
    "<br>\n",
    "\n",
    "* Conditional HETEROSKEDACITY -  where the variance of the error terms is related to the independent variable.  \n",
    "*i.e. if the independent variable is getting bigger and bigger or the variance is increasing and getting bigger.  Maybe, the independent variable is getting smaller and the variance is getting smaller too.*\n",
    "<br>\n",
    "\n",
    "#### NOTES\n",
    "* The model **does not** assume that the dependent variable is uncorrelated with the residuals. \n",
    "* The model **does assume** that the independent variable is uncorrelated with the residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c7902b",
   "metadata": {},
   "source": [
    "### Module 10.1: Linear Regression Basics Quiz\n",
    "<hr>\n",
    "1. What is the most appropriate interpretation of a slope coefficient estimate equal to 10.0?\n",
    "\n",
    "* **Answer:** For every 1-unit change in the independent variable, the model predicts that the dependent variable will change by 10 units.\n",
    " \n",
    "\n",
    "* **Explanation:** The slope coefficient is best interpreted as the predicted change in the dependent variable for a 1-unit change in the independent variable; if the slope coefficient estimate is 10.0 and the independent variable changes by 1 unit, the dependent variable is expected to change by 10 units. The intercept term is best interpreted as the value of the dependent variable when the independent variable is equal to zero. (Module 10.1, LOS 10.a)\n",
    "\n",
    "<hr>\n",
    "\n",
    "2. Which of the following is least likely a necessary assumption of simple linear regression analysis?\n",
    "\n",
    "\n",
    "* **Answer:** The dependent variable is uncorrelated with the residuals.\n",
    "    \n",
    "    \n",
    "* **Explanation:** The model does not assume that the dependent variable is uncorrelated with the residuals. It does assume that the independent variable is uncorrelated with the residuals. (Module 10.1, LOS 10.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80beecd",
   "metadata": {},
   "source": [
    "## Module 10.2: Analysis of Variance (ANOVA) and Goodness of Fit\n",
    "\n",
    "<hr>\n",
    "\n",
    "### LOS 10.c: Calculate and interpret measures of fit and formulate and evaluate tests of fit and of regression coefficients in a simple linear regression.\n",
    "\n",
    "\n",
    "### LOS 10.d: Describe the use of analysis of variance (ANOVA) in regression analysis, interpret ANOVA results, and calculate and interpret the standard error of estimate in a simple linear regression.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Analysis of variance (ANOVA)** is a statistical procedure for analyzing the total variability of the dependent variable. Let's define some terms before we move on to ANOVA tables:\n",
    "\n",
    "* The **total sum of squares (SST)** measures the total variation in the dependent variable. SST is equal to the sum of the squared differences between the actual $Y$-values and the mean of $Y$:\n",
    "\n",
    "$\\qquad\\large\\text{SST}=\\displaystyle\\sum_{i=1}^n(Y_i−\\overline{Y})^2$\n",
    "\n",
    "* The **sum of squares regression (SSR)** measures the variation in the dependent variable that is explained by the independent variable. SSR is the sum of the squared distances between the predicted $Y$-values and the mean of $Y$:\n",
    "\n",
    "$\\qquad\\large\\text{SSR}=\\displaystyle\\sum_{i=1}^n(\\hat{Y}−\\overline{Y})^2$\n",
    "\n",
    "* The **mean square regression (MSR)** is the SSR divided by the number of independent variables. A simple linear regression has only one independent variable, so in this case, MSR = SSR.\n",
    "\n",
    "#### Professor's Note\n",
    "\n",
    "Multiple regression (i.e., with more than one independent variable) is addressed in the Level II CFA curriculum.\n",
    "\n",
    "* The **sum of squared errors (SSE)** measures the unexplained variation in the dependent variable. It's also known as the sum of squared residuals or the residual sum of squares. SSE is the sum of the squared vertical distances between the actual $Y$-values and the predicted $Y$-values on the regression line:\n",
    "<br>\n",
    "\n",
    "$\\qquad\\large\\text{SSR}=\\displaystyle\\sum_{i=1}^n(Y_i−\\hat{Y})^2$\n",
    "\n",
    "</br>\n",
    "\n",
    "* The **mean squared error (MSE)** is the SSE divided by the degrees of freedom, which is $n – 1$ minus the number of independent variables. A simple linear regression has only one independent variable, so in this case, degrees of freedom are $n – 2$.\n",
    "\n",
    "<br>\n",
    "\n",
    "You probably will not be surprised to learn the following:\n",
    "<br>\n",
    "\n",
    "$\\qquad\\text{total variation = explained variation + unexplained variation}$\n",
    "\n",
    "<br>\n",
    "or:\n",
    "\n",
    "<br>\n",
    "$\\qquad\\text{SST = SSR + SSE}$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Components of Total Variation** illustrates how the total variation in the dependent variable (SST) is composed of SSR and SSE.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/PachaTech/CFA-Level-1/blob/main/Module%2010/pics/10_2%20chart.jpeg?raw=true\">\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932763ea",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "The output of the ANOVA procedure is an ANOVA table, which is a summary of the variation in the dependent variable. ANOVA tables are included in the regression output of many statistical software packages. You can think of the ANOVA table as the source of the data for the computation of many of the regression concepts discussed in this reading. A generic ANOVA table for a simple linear regression (one independent variable) is presented in  **ANOVA Table for a Simple Linear Regression**.\n",
    "\n",
    "<img src=\"https://github.com/PachaTech/CFA-Level-1/blob/main/Module%2010/pics/10_2.jpeg?raw=true\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7d13b4",
   "metadata": {},
   "source": [
    "#### Standard Error of Estimate (SEE)\n",
    "\n",
    "The SEE for a regression is the standard deviation of its residuals. The lower the SEE, the better the model fit:\n",
    "\n",
    "$\\qquad\\Large\\text{SEE} = \\sqrt{MSE}$\n",
    "\n",
    "\n",
    "$\\qquad\\Large\\text{SEE} = \\sqrt{\\frac{SSE}{n-2}}$\n",
    "\n",
    "#### Coefficient of Determination (R2)\n",
    "\n",
    "The coefficient of determination (R2) is defined as the percentage of the total variation in the dependent variable explained by the independent variable. For example, an R2 of 0.63 indicates that the variation of the independent variable explains 63% of the variation in the dependent variable:\n",
    "\n",
    "$\\qquad\\Large{R^2 = \\frac{\\text{SSR}}{\\text{SST}}}$\n",
    "\n",
    "#### Professor's Note\n",
    "\n",
    "For simple linear regression (i.e., with one independent variable), the coefficient of determination, $R^2$, may be computed by simply squaring the correlation coefficient, $r$. In other words, $R^2 = r^2$ for a regression with one independent variable.\n",
    "\n",
    "**Example:** Using the ANOVA table\n",
    "\n",
    "Given the following ANOVA table based on 36 observations, calculate the $R^2$ and the standard error of estimate **(SEE)**.\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "\n",
    "<center><b>Completed ANOVA table for ABC regression<\\center><\\b>\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Source of Variation</th>\n",
    "    <th>Degrees of Freedom</th>\n",
    "    <th>Sum of Squares</th>\n",
    "    <th>Mean Sum of Squares</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Regression (explained)</td>\n",
    "    <td>1</td>\n",
    "    <td>0.0076</td>\n",
    "    <td>0.0076</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Error (unexplained)</td>\n",
    "    <td>34</td>\n",
    "    <td>0.0406</td>\n",
    "    <td>0.0012</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Total</td>\n",
    "    <td>35</td>\n",
    "    <td>0.0482</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b055958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input the variables\n",
    "SSR = 0.0076\n",
    "SST = 0.0482\n",
    "MSE = 0.0012\n",
    "# Solve for MSE\n",
    "SSE = 0.0012\n",
    "\n",
    "## Solve for coefficient of determination (R^2)\n",
    "R2 = (SSR / SST)\n",
    "\n",
    "## Calculate SEE\n",
    "SEE = math.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fdacdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variation (SSR)          =   0.0076\n",
      "Total Variation     (SST)          =   0.0482\n",
      "coefficient of determination (MSE) =   0.158\n",
      "coefficient of determination (R^2) =  % 15.8\n",
      "Standard Error of Estimate (SEE)   =   0.035\n"
     ]
    }
   ],
   "source": [
    "## Print variables\n",
    "print(\"Explained Variation (SSR)          =  \", round(SSR, 6))\n",
    "print(\"Total Variation     (SST)          =  \", round(SST, 7))\n",
    "print(\"coefficient of determination (MSE) =  \", round(R2, 3))\n",
    "print(\"coefficient of determination (R^2) =  %\", round(R2*100,1))\n",
    "print(\"Standard Error of Estimate (SEE)   =  \", round(SEE, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729718d8",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "$\\quad R^2=\\frac{\\text{explained variation (SSR)}}{\\text{total variation (SST)}} = \\frac{0.0076}{0.0482}=0.158$ or 15.8% \n",
    "\n",
    "<br>\n",
    "\n",
    "$\\qquad\\text{SEE}=\\sqrt{MSE}=\\sqrt{0.0012}=0.035$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782abd8d",
   "metadata": {},
   "source": [
    "#### The F-Statistic\n",
    "\n",
    "An *F*-test assesses how well a set of independent variables, as a group, explains the variation in the dependent variable.\n",
    "\n",
    "The *F*-statistic is calculated as follows:\n",
    "\n",
    "<br>\n",
    "$\\quad\\large\\text{F}=\\frac{\\text{MSR}}{\\text{MSE}}=\\frac{\\text{SSR}\\div K}{\\text{SSE}\\div(n-k-1)}$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\text{MSR} =$ mean regression sum of squares\n",
    "\n",
    "$\\text{MSE} =$ mean squared error\n",
    "\n",
    "**Important**: This is always a one-tailed test\n",
    "\n",
    "<br>\n",
    "For simple linear regression, there is only one independent variable, so the $F$-test is equivalent to a $t$-test of the statistical significance of the slope coefficient:\n",
    "\n",
    "$\\qquad H_0: b_1 = 0$   versus   $H_a: b_1 \\neq 0$\n",
    "\n",
    "To determine whether $b_1$ is statistically significant using the $F$-test, the calculated $F$-statistic is compared with the critical $F$-value, $F_c$, at the appropriate level of significance. The degrees of freedom for the numerator and denominator with one independent variable are as follows:\n",
    "\n",
    "$\\qquad df_{\\text{numerator}} = k = 1$\n",
    "\n",
    "$\\qquad df_{\\text{denominator}} = n − k − 1 = n − 2$\n",
    "\n",
    "where:\n",
    "\n",
    "$n =$ number of observations\n",
    "\n",
    "The decision rule for the $F$-test is to reject $H_0$ if $F > F_c$.\n",
    "\n",
    "<br>\n",
    "\n",
    "Rejecting the null hypothesis that the value of the slope coefficient equals zero at a stated level of significance indicates that the independent variable and the dependent variable have a significant linear relationship.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a3f1cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Test Decision Rule (F)   =   6.33\n"
     ]
    }
   ],
   "source": [
    "## Define variables\n",
    "k = 1\n",
    "n = -2\n",
    "\n",
    "# Solve the F-test\n",
    "MSR = SSR / k\n",
    "F = MSR / MSE \n",
    "# MSE = SEE / (n-k-1) \n",
    "\n",
    "\n",
    "## print(k)\n",
    "## print(n)\n",
    "## print(MSR)\n",
    "## print(MSE)\n",
    "print(\"F-Test Decision Rule (F)   =  \", round(F, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275470b",
   "metadata": {},
   "source": [
    "**Example**: Calculating and interpreting the $F$-statistic\n",
    "\n",
    "Use the ANOVA table from the previous example to calculate and interpret the F-statistic. Test the null hypothesis at the 5% significance level that the slope coefficient is equal to 0.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "$\\qquad\\Large F=\\frac{\\text{MSR}}{\\text{MSE}}= \\frac{0.0076}{0.0012}=6.33$\n",
    "\n",
    "$\\text{df}_\\text{numerator} = k = 1$\n",
    "\n",
    "$\\text{df}_\\text{denominator} = n − k − 1 = 36 − 1 − 1 = 34$\n",
    "\n",
    "\n",
    "The null and alternative hypotheses are $h_0:b_1 = 0$ versus $h_a:b_1 \\neq 0$. \n",
    "The critical $F$-value for $1$ and $34$ degrees of freedom at a $5\\%$ significance level is approximately $4.1$. \n",
    "(Remember, it's a one-tailed test, so we use the $5\\%$ $F$-table.) \n",
    "Therefore, we can reject the null hypothesis and conclude that the slope coefficient is significantly different than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4622f",
   "metadata": {},
   "source": [
    "**Hypothesis Test of a Regression Coefficient**\n",
    "\n",
    "A t-test may also be used to test the hypothesis that the true slope coefficient, $b_1$, is equal to a hypothesized value. Letting $\\hat{b}_1$ be the point estimate for $b_1$, the appropriate test statistic with $n − 2$ degrees of freedom is:\n",
    "\n",
    "$\\qquad  tb_1= \\Large\\frac{\\hat{b}_1 - {b}_1}{S\\hat{b}_1} $\n",
    "\n",
    "The decision rule for tests of significance for regression coefficients is:\n",
    "\n",
    "Reject $H_0$ if $t > + $ $t_\\text{critical}$ or $t < -$ $t_\\text{critical}$\n",
    "\n",
    "Rejection of the null supports the alternative hypothesis that the slope coefficient is different from the hypothesized value of $b_1$. To test whether an independent variable explains the variation in the dependent variable (i.e., it is statistically significant), the null hypothesis is that the true slope is zero $(b_1 = 0)$. The appropriate test structure for the null and alternative hypotheses is:\n",
    "\n",
    "$H_0: b_1 = 0$ versus  $H_a:b_1 \\neq 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f2150",
   "metadata": {},
   "source": [
    "**Example: Hypothesis test for significance of regression coefficients**\n",
    "\n",
    "The estimated slope coefficient from the ABC example is $0.64$ with a standard error equal to $0.26$. Assuming that the sample has $36$ observations, determine if the estimated slope coefficient is significantly different than zero at a $5\\%$ level of significance.\n",
    "\n",
    "$\\color{red}{\\text{NOTE: Use the T-table in the back of the book for the observations and variables}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a2efec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t =  2.46\n"
     ]
    }
   ],
   "source": [
    "## Define Variables\n",
    "bhat = 0.64\n",
    "b1 = 0\n",
    "s_b1 = 0.26\n",
    "\n",
    "\n",
    "## Solve for t\n",
    "t = (bhat - b1) / s_b1\n",
    "\n",
    "## Print Output\n",
    "print(\"t = \", round(t, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb94cf",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "The calculated test statistic is:\n",
    "\n",
    "$\\Large t=\\Large\\frac{\\hat{b}_1-b_1}{S_{\\hat{b}_1}}= \\frac{0.64-0}{0.26}= 2.46$\n",
    "\n",
    "The critical two-tailed $t$-values are  $\\pm2.03$ (from the $t$-table with df = 36 − 2 = 34). Because $t > \\text{t}_\\text{critical}$ (i.e., $2.46 > 2.03$), we reject the null hypothesis and conclude that the slope is different from zero.\n",
    "\n",
    "Note that the $t$-test for a simple linear regression is equivalent to a $t$-test for the correlation coefficient between $x$ and $y$:\n",
    "\n",
    "$\\qquad\\Large t_2 = \\Large\\frac{r\\sqrt{n-2}}{\\sqrt{1-r2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c17dd4",
   "metadata": {},
   "source": [
    "## Module 10.2: Analysis of Variance (ANOVA) and Goodness of Fit Quiz\n",
    "<HR>\n",
    "    \n",
    "1. Consider the following statement: \"In a simple linear regression, the appropriate degrees of freedom for the critical t-value used to calculate a confidence interval around both a parameter estimate and a predicted Y-value is the same as the number of observations minus two\". This statement is:\n",
    "    \n",
    "    * **Answer:** justified\n",
    "    * **Explanation:** In simple linear regression, the appropriate degrees of freedom for both confidence intervals is the number of observations in the sample (n) minus two. (Module 10.2, LOS 10.c)\n",
    "\n",
    "<br>    \n",
    "    \n",
    "2. What is the appropriate alternative hypothesis to test the statistical significance of the intercept term in the following regression?\n",
    "$\\qquad Y = a_1 + a_2(X) + \\epsilon$\n",
    "    \n",
    "    * **Answer:** $H_A: a_1 \\neq 0$.\n",
    "    * **Explanation:** In this regression, a1 is the intercept term. To test the statistical significance means to test the null hypothesis that a1 is equal to zero, versus the alternative that a1 is not equal to zero. (Module 10.2, LOS 10.c)\n",
    "\n",
    "<br>    \n",
    "    \n",
    "3. The variation in the dependent variable explained by the independent variable is measured by the:\n",
    "    \n",
    "    * **Answer:** regression sum of squares.\n",
    "    * **Explanation:** The regression sum of squares measures the amount of variation in the dependent variable explained by the independent variable (i.e., the explained variation). The sum of squared errors measures the variation in the dependent variable not explained by the independent variable. The mean squared error is equal to the sum of squared errors divided by its degrees of freedom. (Module 10.2, LOS 10.d)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff352364",
   "metadata": {},
   "source": [
    "## Module 10.3: Predicted Values and Functional Forms of Regression\n",
    "<hr>\n",
    "\n",
    "### LOS 10.e: Calculate and interpret the predicted value for the dependent variable, and a prediction interval for it, given an estimated linear regression model and a value for the independent variable.\n",
    "\n",
    "**Predicted values** are values of the dependent variable based on the estimated regression coefficients and a prediction about the value of the independent variable. They are the values that are predicted by the regression equation, given an estimate of the independent variable.\n",
    "\n",
    "For a simple regression, this is the predicted (or forecast) value of $Y$:\n",
    "\n",
    "$\\hat{Y}=\\hat{b}_0+\\hat{b}_1 X_p$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\hat{Y} =$ predicted value of the dependent variable\n",
    "\n",
    "$X_p =$ forecasted value of the independent variable\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Example:** Predicting the dependent variable\n",
    "\n",
    "Given the ABC regression equation as follows:\n",
    "\n",
    "$\\widehat{ABC} = −2.3\\% + (0.64) (\\widehat{SP500}) $\n",
    "\n",
    "Calculate the predicted value of ABC excess returns if forecast S&P 500 excess returns are 10%.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The predicted value for ABC excess returns is determined as follows:\n",
    "\n",
    "$\\widehat{ABC} = −2.3\\% + (0.64)(10\\%) = 4.1\\%$\n",
    "\n",
    "\n",
    "\n",
    "#### Confidence Intervals for Predicted Values\n",
    "\n",
    "This is the equation for the confidence interval for a predicted value of Y:\n",
    "\n",
    "$\\hat{Y}\\pm (t_c \\times s_f)\\Rightarrow [\\hat{Y}(t_c \\times s_f) < Y < \\hat{Y} + (t_c \\times s_f)]$\n",
    "\n",
    "where:\n",
    "\n",
    "$t_c$ = two-tailed critical t-value at the desired level of significance with df = n − 2\n",
    "\n",
    "$s_f$ = standard error of the forecast\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4112cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The forecast for ABC excess returns is:  4.1 %\n"
     ]
    }
   ],
   "source": [
    "## Define Forecast Values for equities\n",
    "abc_hat = -0.023 + (bhat6 * 0.1) \n",
    "print(\"The forecast for ABC excess returns is: \",(round(abc_hat*100, 8)),\"%\")## Calculate formula "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4293c4",
   "metadata": {},
   "source": [
    "<mark>\n",
    "$\\color{blue}{\\text{The intercept (-2.3%) & Slope coefficient (0.64) are sample values}}$\n",
    "    \n",
    " * **two sources of error**.\n",
    "     1. the missing error term\n",
    "     2. slope & intercept are sample, not population values.\n",
    "        * Confidence Intervals\n",
    "        * Standard error of the estimate <b>SEE</b>\n",
    "        * The <b>SEE</b> is the standard deviation of the error term\n",
    "\n",
    "\n",
    "The challenge with computing a <u>confidence interval</u> for a predicted value is calculating $s_f$. \n",
    "On the Level I exam, it's highly unlikely that you will have to calculate the standard error of the forecast (it <b>will probably be provided</b> if you need to compute a confidence interval for the dependent variable). However, if you do need to calculate sf, it can be done with the following formula for the variance of the forecast:\n",
    "    \n",
    "##### Confidence Error    \n",
    "\n",
    "$ \\hat{Y}\\pm(t_c \\times s_f)   \\longleftarrow   s^2_f=\\text{SEE}^2 \\Biggr[1+\\large\\frac{1}{n}+(X−\\overline{X})^2\\large\\frac{(n−1)}{s^2_x}\\Biggr]  $\n",
    "\n",
    "    \n",
    "where:\n",
    "\n",
    "$\\text{SEE}^2 =$ variance of the residuals = the square of the standard error of estimate\n",
    "\n",
    "$s^2_x =$ variance of the independent variable\n",
    "\n",
    "$X =$ value of the independent variable for which the forecast was made\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Example:** Confidence interval for a predicted value\n",
    "\n",
    "Calculate a $95\\%$ prediction interval on the predicted value of ABC excess returns from the previous example. Assume the standard error of the forecast is $3.67$, and the forecast value of S&P 500 excess returns is $10\\%$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "This is the predicted value for ABC excess returns:\n",
    "\n",
    "$\\widehat{ABC}=−2.3\\%+(0.64)(10\\%)=4.1\\%$\n",
    "\n",
    "The 5% two-tailed critical t-value with 34 degrees of freedom is 2.03. This is the prediction interval at the 95% confidence level:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0d82293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The forecast for ABC Confidence Interval returns is:  4.174501 %\n"
     ]
    }
   ],
   "source": [
    "#ABC HAT Confidence intervals Returns\n",
    "abc_hat2 = (abc_hat) + 0.0203 * 0.0367\n",
    "print(\"The forecast for ABC Confidence Interval returns is: \",(round(abc_hat2*100, 10)),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c6dd78",
   "metadata": {},
   "source": [
    "$\\widehat{ABC}\\pm(t_c \\times s_f)\\Rightarrow[4.1\\% \\pm, (2.03×3.67\\%)]=4.1\\% \\pm 7.5\\%$\n",
    "\n",
    "Or, $–3.4\\%$ to $11.6\\%$.\n",
    "\n",
    "We can interpret this range to mean that, given a forecast value for S&P 500 excess returns of $10\\%$, we can be $95\\%$ confident that the ABC excess returns will be between $–3.4\\%$ and $11.6\\%$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec03a6",
   "metadata": {},
   "source": [
    "### Types of Models\n",
    "In such a situation, transforming one or both of the variables can produce a linear relationship. The appropriate transformation depends on the relationship between the two variables. One often-used transformation is to take the natural log of one or both of the variables. Here are some examples:\n",
    "\n",
    "* **Log-lin model**. This is if the dependent variable is logarithmic, while the independent variable is linear.\n",
    "\n",
    "\n",
    "* **Lin-log model**. This is if the dependent variable is linear, while the independent variable is logarithmic.\n",
    "\n",
    "\n",
    "* **Log-log model**. Both the dependent variable and the independent variable are logarithmic.\n",
    "\n",
    "\n",
    "Selecting the correct functional form involves determining the nature of the variables and evaluating the goodness-of-fit measures (e.g., R2, SEE, F-stat).\n",
    "\n",
    "\n",
    "#### Log-Lin Model\n",
    "\n",
    "Taking the natural logarithm of the dependent variable, our model now becomes this:\n",
    "\n",
    "$\\ln Y_i = b_0 + b_1(X)_i + \\epsilon_i$\n",
    "\n",
    "In this model, the slope coefficient is interpreted as the relative change in dependent variable for an absolute change in the independent variable.  Log-Lin Model, EPS Data shows the results after taking the natural log of EPS, and fitting that data using a log-lin model.\n",
    "\n",
    "* **Log-lin model**. This is if the dependent variable is logarithmic, while the independent variable is linear.\n",
    "\n",
    "#### Log-Log Model\n",
    "\n",
    "Taking the natural logarithm of both variables, our model now becomes this:\n",
    "\n",
    "$\\ln Y_i = b_0 + b_1\\ln(X)_i + \\epsilon_i$\n",
    "\n",
    "In this model, the slope coefficient is interpreted as the relative change in dependent variable for a relative change in the independent variable.\n",
    "\n",
    "* **Log-log model**. Both the dependent variable and the independent variable are logarithmic.\n",
    "\n",
    "\n",
    "\n",
    "#### Lin-Log Model\n",
    "\n",
    "Taking the natural logarithm of the independent variable, our model now becomes this:\n",
    "\n",
    "$Y_i = b_0 + b_1\\ln(X)_i + \\epsilon_i$\n",
    "\n",
    "In this model, the slope coefficient is interpreted as the absolute change in dependent variable for a relative change in the independent variable.\n",
    "\n",
    "\n",
    "* **Lin-log model**. This is if the dependent variable is linear, while the independent variable is logarithmic.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
